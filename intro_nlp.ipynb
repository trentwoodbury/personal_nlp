{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "from BeautifulSoup import BeautifulSoup\n",
    "from collections_extended import setlist\n",
    "from itertools import permutations\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk import *\n",
    "import numpy as np\n",
    "import os\n",
    "import re, pprint\n",
    "from urllib import urlopen\n",
    "import unidecode\n",
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordnet\n",
    "The wordnet is a hierarchy of words. It is defined by a set of 3 relationships between words.<br><br>\n",
    "<b>Hyponyms</b> are parts of a larger piece (e.g. face is a hyponym of body)<br>\n",
    "<b>Hypernyms</b> are the larger piece of something (e.g. house is a hypernym of room)<br>\n",
    "<b>Root Hypernym</b> is the absolute top of the wordnet for a word (e.g. entity is the root hypernym of dog)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma: [u'train', u'railroad_train']\n",
      "hyponyms: [Synset('boat_train.n.01'), Synset('car_train.n.01'), Synset('freight_train.n.01'), Synset('hospital_train.n.01'), Synset('mail_train.n.01'), Synset('passenger_train.n.01'), Synset('streamliner.n.01'), Synset('subway_train.n.01')]\n",
      "hypernyms: [Synset('public_transport.n.01')]\n",
      "root hypernym: [Synset('entity.n.01')] \n",
      "\n",
      "lemma: [u'string', u'train']\n",
      "hyponyms: []\n",
      "hypernyms: [Synset('series.n.01')]\n",
      "root hypernym: [Synset('entity.n.01')] \n",
      "\n",
      "lemma: [u'caravan', u'train', u'wagon_train']\n",
      "hyponyms: []\n",
      "hypernyms: [Synset('procession.n.02')]\n",
      "root hypernym: [Synset('entity.n.01')] \n",
      "\n",
      "lemma: [u'train']\n",
      "hyponyms: []\n",
      "hypernyms: [Synset('consequence.n.02')]\n",
      "root hypernym: [Synset('entity.n.01')] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_count = 0\n",
    "for synset in corpus.wordnet.synsets('train'):\n",
    "    if print_count > 3:\n",
    "        break\n",
    "    print_count += 1\n",
    "    print \"lemma:\", synset.lemma_names()\n",
    "    print \"hyponyms:\", synset.hyponyms()\n",
    "    print \"hypernyms:\", synset.hypernyms()\n",
    "    print \"root hypernym:\", synset.root_hypernyms(), \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('walk.n.01')\n",
      "part meronyms:  [Synset('pace.n.04')]\n",
      "substance meronyms:  []\n",
      "part holonyms:  []\n",
      "Entailments:  [] \n",
      "\n",
      "Synset('base_on_balls.n.01')\n",
      "part meronyms:  []\n",
      "substance meronyms:  []\n",
      "part holonyms:  []\n",
      "Entailments:  [] \n",
      "\n",
      "Synset('walk.n.03')\n",
      "part meronyms:  []\n",
      "substance meronyms:  []\n",
      "part holonyms:  []\n",
      "Entailments:  [] \n",
      "\n",
      "Synset('walk.n.04')\n",
      "part meronyms:  []\n",
      "substance meronyms:  []\n",
      "part holonyms:  []\n",
      "Entailments:  [] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_count = 0\n",
    "for synset in corpus.wordnet.synsets('walk'):\n",
    "    if print_count > 3:\n",
    "        break\n",
    "    print_count += 1\n",
    "    print synset    \n",
    "    print \"part meronyms: \",  synset.part_meronyms()\n",
    "    print \"substance meronyms: \",  synset.substance_meronyms()\n",
    "    print \"part holonyms: \",  synset.part_holonyms()\n",
    "    print \"Entailments: \", synset.entailments(), \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in text from a webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "# pull in text and convert it to utf-8. The .decode is unnecessary in Python 3.\n",
    "raw = urlopen(url).read().decode('utf-8')\n",
    "tokens = word_tokenize(raw)\n",
    "text = Text(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This text is now in nltk text format. To get a list of all the words we can simply use a list comprehension.<br>\n",
    "Something that's nice about the nltk text object is that we can use NLTK functions on the text. For example, we can look at <b>collocations</b>, words that occur together frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text collocations:Katerina Ivanovna; Pyotr Petrovitch; Pulcheria Alexandrovna; Avdotya\n",
      "Romanovna; Rodion Romanovitch; Marfa Petrovna; Sofya Semyonovna;\n",
      "Project Gutenberg-tm; old woman; Porfiry Petrovitch; great deal;\n",
      "Amalia Ivanovna; don’t know; Nikodim Fomitch; young man; Andrey\n",
      "Semyonovitch; Hay Market; Dmitri Prokofitch; Ilya Petrovitch; Katerina\n",
      "Ivanovna’s\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "text_list = [w for w in text]\n",
    "print \"text collocations:\", text.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>^</b> denotes the beginning of a word. <br>\n",
    "<b>.</b> is a wildcard character <br>\n",
    "<b>$</b> denotes the end of a word.<br>\n",
    "<b>?</b> means that the previous character is optional. <br>\n",
    "<b>[abc]</b> means that the character in this space can be any of a, b, or c. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'abjectly', u'adjuster', u'dejected', u'dejectly', u'injector', u'majestic', u'objectee', u'objector', u'rejecter', u'rejector']\n"
     ]
    }
   ],
   "source": [
    "wordlist = [w for w in nltk.corpus.words.words('en') if w.islower()]\n",
    "# print words with j and t in the middle of length 8\n",
    "print [w for w in wordlist if re.search('^..j..t..$', w)][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'intervention', u'invention']\n"
     ]
    }
   ],
   "source": [
    "print [w for w in wordlist if re.search('^int?e?r?vention$', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'adglutinate', u'afikomen', u'beglad', u'beglamour', u'beglare', u'beglerbeg', u'beglerbeglic', u'beglerbegluc', u'beglerbegship', u'beglerbey']\n"
     ]
    }
   ],
   "source": [
    "print [w for w in wordlist if re.search('^[abc][def][ghi][jkl]', w)][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>+</b> means 1 or more instances of the item<br>\n",
    "<b>\\*</b> means 0 or more instances of an item<br>\n",
    "+, * are refered to as <b>closures</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'po', u'poaceous', u'poach', u'poachable', u'poacher', u'poachiness', u'poachy', u'poalike', u'pob', u'pobby']\n"
     ]
    }
   ],
   "source": [
    "print [w for w in wordlist if re.search('^p+o+p*', w)][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>^</b> inside a bracket matches to any character not inside the bracket.<br>\n",
    "In the example below, we see words with no vowels in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'b', u'by', u'byth', u'c', u'cly', u'cry', u'crypt', u'cwm', u'cyp', u'cyst']\n"
     ]
    }
   ],
   "source": [
    "print [w for w in wordlist if re.search('^[^aeiouAEIOU]+$', w)][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>|</b> is refered to as the disjunction. It is an OR operator. <br>\n",
    "<b>{n}</b> specifies exactly n repeats. <br>\n",
    "<b>{n,}</b> specifies at least n repeats. <br>\n",
    "<b>{,n}</b> specifies at most n repeats.<br>\n",
    "<b>{n,m}</b> specifies between n and m repeats <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'blype', u'bryaceous', u'bryogenin', u'bryological', u'bryologist', u'bryology', u'bryonidin', u'bryonin', u'bryony', u'bryophyte']\n"
     ]
    }
   ],
   "source": [
    "# words with at least 3 non-vowels at the beginning\n",
    "print [w for w in wordlist if re.search('^[^aeiouAEIOU]{3,}', w)][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'a', u'aa', u'aal', u'aalii', u'aam', u'aardvark', u'aardwolf', u'aba', u'abac', u'abaca']\n"
     ]
    }
   ],
   "source": [
    "# words with no more than 3 consonants at the beginning\n",
    "print [w for w in wordlist if re.search('^[^aeiouAEIOU]{,3}', w)][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'blype', u'byplay', u'byrlaw', u'byrlawman', u'byrnie', u'byrrus', u'byrthynsak', u'bysmalith', u'byspell', u'byssaceous']\n"
     ]
    }
   ],
   "source": [
    "# words with between 4 and 6 consonants at the beginning\n",
    "print [w for w in wordlist if re.search('^[^aeiouAEIOU]{4,6}', w)][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'accosted', u'arresting', u'ballasting', u'basting', u'beforested', u'billposting', u'bilsted', u'blasted', u'blasting', u'bloodthirsting']\n"
     ]
    }
   ],
   "source": [
    "# words that end with 'sting' or 'sted'\n",
    "# () tell us the scope of the operator\n",
    "print [w for w in wordlist if re.search('(sting|sted)$', w)][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Regular Expressions for Texts (as opposed to just words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use regular expressions across words. Here we can interact with an NLTK Text object to find instances of a regular expression. Because NLTK's built in findall function prints results instead of returning them (why?), I have defined my own function that returns the results instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_regex_token_match(regexp, nltk_text):\n",
    "    '''\n",
    "    NLTK's finall method prints the results instead of returning them.\n",
    "    Since this is worthless for any real world application, I have \n",
    "    tweaked their source code to return instead of print the results\n",
    "    by using this function instead of nltk.find_all.\n",
    "    INPUT\n",
    "        nltk_text: an NLTK text object\n",
    "        regexp: a resular expression\n",
    "    OUTPUT\n",
    "        match_list: a list of tokens\n",
    "    '''\n",
    "    if \"_token_searcher\" not in nltk_text.__dict__:\n",
    "            nltk_text._token_searcher = nltk.TokenSearcher(nltk_text)\n",
    "    hits = nltk_text._token_searcher.findall(regexp)\n",
    "    return [' '.join(h) for h in hits]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at words used to describe men and women in Moby Dick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Men Descriptions: {[u'artificial', u'any', u'that', u'a', u'monied', u'nervous', u'old', u'decent', u'This', u'this', u'No', u'the', u'dangerous', u'white', u'one', u'That', u'every', u'worsted', u'faithful', u'Miserable', u'honest', u'is', u'fellow', u'no', u'first', u'elderly', u'!', u'young', u'Young', u'pious', u'our', u'impenitent', u',', u'queer', u'like', u'good', u'crazy', u'of', u'mature', u'earnest', u'steadfast', u'fearless', u'mighty', u'but', u'ruined', u'-', u'unfearing', u'Cape', u'sepulchral', u'other', u'sleeping', u'less', u'little', u'great', u'wise', u'better', u'by', u'are', u'from', u'handed', u'and', u'butterless', u'meditative', u'If', u'mortal', u'to', u'very', u'fiendish', u'for', u'Albino', u'pale', u'spiritual', u'manufactured', u'Every', u'dead', u'last', u'legs', u'maimed', u'best', u'each', u'monomaniac', u'Any', u'infatuated', u'your', u'their', u'Teneriffe', u'furious', u'half', u'experienced', u'baby', u'killed', u'brack', u'single', u'O', u'timid', u'youngish', u'certain', u'drowned', u'same', u'than', u'in', u'stoutest', u'In', u'with', u'common', u'looking', u'though', u'yet', u'So', u'cases', u'ornamented', u'legged', u'abstinence', u'learned', u'per', u'untravelled', u'abstracted', u'complete', u'dismasted', u'younger', u'sick', u'civilized', u'cast', u'Old', u'brave', u'upright', u'suffering', u'only', u'nor', u'between', u'third'}] \n",
      "\n",
      "Women Descriptions: {[u'old', u'freckled', u'any', u'mortal', u'a'}]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg, nps_chat\n",
    "moby = nltk.Text(gutenberg.words('melville-moby_dick.txt'))\n",
    "\n",
    "\n",
    "man_descriptions = create_regex_token_match(r\"(<.*>)<man>\", moby)\n",
    "woman_descriptions = create_regex_token_match(r\"(<.*>)<woman>\", moby)\n",
    "print \"Men Descriptions:\", setlist(man_descriptions), \"\\n\"\n",
    "print \"Women Descriptions:\", setlist(woman_descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at plural words related by \"and\" within the brown text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'lats', u'serratus'), (u'judges', u'audiences'), (u'rhythms', u'infectious'), (u'swoops', u'slides'), (u'parents', u'guests'), (u'hands', u'legs'), (u'us', u'has'), (u'lakes', u'reservoirs'), (u'Designers', u'manufacturers'), (u'Terms', u'rates')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "hobbies_learned = nltk.Text(brown.words(categories=['hobbies', 'learned']))\n",
    "related_hobbies = create_regex_token_match(r\"<\\w*s> <and> <\\w*s>\", hobbies_learned)\n",
    "print [(val.split(\" \")[0], val.split(\" \")[-1]) for val in related_hobbies[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
