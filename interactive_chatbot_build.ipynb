{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is this notebook?\n",
    "This notebook takes in movie conversations and trains a chatbot to interact with people. This chatbot is trained via a tensorflow neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.1: Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_in_lines():\n",
    "    with open('data/dialogues/movie_lines.txt', 'r') as f:\n",
    "        lines = f.read().split('\\n')\n",
    "    with open('data/dialogues/movie_conversations.txt', 'r') as g:\n",
    "        conv_lines = g.read().split('\\n')\n",
    "    return lines, conv_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_line_mapping(lines):\n",
    "    '''\n",
    "    creates a dictionary mapping lineids to the lines.\n",
    "    '''\n",
    "    line_mapping = {}\n",
    "    for line in lines:\n",
    "        split_line = line.split(' +++$+++ ')\n",
    "        if len(split_line) == 5:\n",
    "            line_mapping[split_line[0]] = split_line[4]\n",
    "    return line_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_conv_lines(conv_lines):\n",
    "    '''\n",
    "    extracts just the line ids from the conv_lines list.\n",
    "    returns a list of lists of strings. Each string is a lineid.\n",
    "    '''\n",
    "    convs = []\n",
    "    for conv in conv_lines[:-1]:\n",
    "        conv_line_list = conv.split(\" +++$+++ \")[-1]\n",
    "        convs.append(ast.literal_eval(conv_line_list))\n",
    "    return convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_questions_and_answers(convs, line_mapping):\n",
    "    '''\n",
    "    creates a list of questions and a list of answers.\n",
    "    These are the back to back lines from each conversation.\n",
    "    '''\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for conv in convs:\n",
    "        for i in range(len(conv) - 1):\n",
    "            questions.append(line_mapping[conv[i]])\n",
    "            answers.append(line_mapping[conv[i+1]])\n",
    "    return questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clean_questions_and_answers(questions, answers):\n",
    "    '''\n",
    "    applies clean_text function to questions and answers lists.\n",
    "    '''\n",
    "    clean_questions = [clean_text(text) for text in questions]\n",
    "    clean_answers = [clean_text(text) for text in answers]\n",
    "    return clean_questions, clean_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_long_and_short_sentences(\n",
    "        questions, \n",
    "        answers, \n",
    "        min_sentence_length=2, \n",
    "        max_sentence_length=20):\n",
    "    '''\n",
    "    Filters out questions and answers with length below the min_threshold (2)\n",
    "    and above the max_threshold (20)\n",
    "    '''\n",
    "    questions_filtered = []\n",
    "    answers_filtered = []\n",
    "    \n",
    "    for q, a in zip(questions, answers):\n",
    "        if len(q.split()) > min_sentence_length and len(q.split()) < max_sentence_length and \\\n",
    "        len(a.split()) > min_sentence_length and len(a.split()) < max_sentence_length:\n",
    "            questions_filtered.append(q)\n",
    "            answers_filtered.append(a)\n",
    "   \n",
    "    return questions_filtered, answers_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_frequency_dictionaries(sentence_list):\n",
    "    '''\n",
    "    This function applies to either the list of questions of the list of answers.\n",
    "    It creates a dictionary mapping words to the number of times that word occurs.\n",
    "    '''\n",
    "    word_frequencies = {}\n",
    "    for sentence in sentence_list:\n",
    "        for word in sentence.split():\n",
    "            if word_frequencies.get(word, 0) == 0:\n",
    "                word_frequencies[word] = 1\n",
    "            else:\n",
    "                word_frequencies[word] += 1\n",
    "    return word_frequencies    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_uncommon_words(vocab, threshold):\n",
    "    '''\n",
    "    filters out words from vocab dictionary (output of create_word_frequency_dictionaries)\n",
    "    that occurs fewer than threshold many times.\n",
    "    '''\n",
    "    filtered_vocab = {key: val for key, val in vocab.iteritems() if val >= threshold}\n",
    "    return filtered_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unique_identifiers(vocab):\n",
    "    '''\n",
    "    creates dictionary mapping each word in vocab to a unique number\n",
    "    '''\n",
    "    words = set(vocab.keys())\n",
    "    vocab_identifiers = {}\n",
    "    unique_int = 0\n",
    "    for word in words:\n",
    "        vocab_identifiers[word] = unique_int\n",
    "        unique_int += 1\n",
    "    \n",
    "    return vocab_identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary_tokens(vocab_identifiers):\n",
    "    '''\n",
    "    adds identification tokens to the vocab_identifiers dictionary\n",
    "    '''\n",
    "    codes = ['<PAD>','<EOS>','<UNK>','<GO>']\n",
    "\n",
    "    for code in codes:\n",
    "        vocab_identifiers[code] = len(filtered_vocab)+1\n",
    "    \n",
    "    # Rename vocab_identifiers now this has tokens. This is just for interpretability reasons.\n",
    "    identifiers_with_tokens = vocab_identifiers\n",
    "    return identifiers_with_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_int_to_word_dict(identifiers_with_tokens):\n",
    "    '''\n",
    "    creates a dictionary mapping integers to words as opposed to words mapping to \n",
    "    integers.\n",
    "    '''\n",
    "    int_to_word_dict = {idx : word for word, idx in identifiers_with_tokens.iteritems()}\n",
    "    return int_to_word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_eos_token(lines):\n",
    "    '''\n",
    "    Here lines is a list of strings (sentences). This will generally be either\n",
    "        questions_filtered (or)\n",
    "        answers_filtered\n",
    "    but any input of form list of strings will word.\n",
    "    This function appends ' <EOS>' to the end of each string, representing the\n",
    "    end of sentence.\n",
    "    '''\n",
    "    lines_with_eos = [x + ' <EOS>' for x in lines]\n",
    "    return lines_with_eos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lines_to_ints(lines, vocab_identifiers):\n",
    "    '''\n",
    "    converts list of lines to a list of list of integers where each integer represents\n",
    "    a word. The integer mapping is per the vocab_identifiers defined by the \n",
    "    create_unique_identifiers function. If a word isn't in the vocab_identifiers, it will be\n",
    "    replaced with '<UNK>' for unknown.\n",
    "    INPUT\n",
    "        lines: list of strings. Each string is a movie line.\n",
    "        vocab_identifiers: dictionary. maps words to unique identifiers. \n",
    "    OUTPUT\n",
    "        int_lines: list of list of integers.\n",
    "    '''\n",
    "    int_lines = []\n",
    "    for line in lines:\n",
    "        word_split = line.split()\n",
    "        words_as_ints = map(lambda x: vocab_identifiers.get(x, '<UNK>'), word_split)\n",
    "        int_lines.append(words_as_ints)\n",
    "    \n",
    "    return int_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percent_unk(int_lines):\n",
    "    '''\n",
    "    tells us the percent of total words in int_lines that are the '<UNK>' token.\n",
    "    INPUT\n",
    "        int_lines: list of list of integers and <UNK> tokens. Output of convert_lines_to_ints\n",
    "        function.\n",
    "    '''\n",
    "    words_flat = [word for sentence in int_lines for word in sentence]\n",
    "    word_set = set(words_flat)\n",
    "    with_unk = len(words_flat)\n",
    "    without_unk = len([x for x in words_flat if type(x) == int])\n",
    "    percent_unk = (with_unk - float(without_unk)) / with_unk\n",
    "    \n",
    "    print \"{} unique words\".format(len(word_set))\n",
    "    print \"Percent of total words spoken that are unkown: {}%\".format(round(100*percent_unk, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_question_length(int_form_questions, int_form_answers):\n",
    "    '''\n",
    "    Sorts question and answer pairs by the lengths of the questions.\n",
    "    Implementing this function speeds up training time and reduces loss.\n",
    "    INPUT\n",
    "        int_form_questions: list of integers and <UNK> tokens, the questions.\n",
    "        int_form_answers: list of integers and <UNK> tokens, the answers.\n",
    "    OUTPUT\n",
    "        int_questions_sorted: sorted list of integers and <UNK> tokens, the questions.\n",
    "        int_answers_sorted: sorted list of integers and <UNK> tokens, the answers.\n",
    "    '''\n",
    "    indices = np.argsort([len(sent) for sent in int_form_questions])\n",
    "    int_questions_sorted = list(np.array(int_form_questions)[indices])\n",
    "    int_answers_sorted = list(np.array(int_form_answers)[indices])\n",
    "    return int_questions_sorted, int_answers_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, vocab_identifiers):\n",
    "    '''\n",
    "    Pads every sentence in a batch so that all sentences have the same length.\n",
    "    INPUT\n",
    "        sentence_batch: numpy array of integers. Batch of sentences.\n",
    "        vocab_identifiers: dictionary mapping vocabulary to unique integers.\n",
    "    OUTPUT\n",
    "        padded_sentence_batch: list of integers padded by integer representation\n",
    "            for <PAD>.\n",
    "    '''\n",
    "    max_length = max([len(sentence) for sentence in sentenc_batch])\n",
    "    padded_sentence_batch = np.array([\n",
    "        sentence + vocab_identifiers['<PAD>']*(max_length - len(sentence)) \n",
    "        for sentence in sentence_batch\n",
    "    ])\n",
    "    return padded_sentence_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(int_form_questions, int_form_answers, batch_size, vocab_identifiers):\n",
    "    '''\n",
    "    Create batches, pairing up questions and answers.\n",
    "    INPUT\n",
    "        int_form_questions: list of integers (representing words), the questions.\n",
    "        int_form_answers: list of integers (representing words), the answers.\n",
    "        batch_size: integer. number of sentences to be in each batch.\n",
    "        vocab_identifiers: dictionary mapping vocabulary to unique integers.\n",
    "    OUTPUT\n",
    "        YIELDS padded_question_batch: numpy array of padded integer represented sentences.\n",
    "        YIELDS padded_answer_batch numpy array of padded integer represented sentences.\n",
    "    '''\n",
    "    for batch_i in range(0, len(questions) // batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        \n",
    "        questions_batch = int_form_questions[start_i : start_i + batch_size]\n",
    "        answers_batch = int_form_answers[start_i : start_i + batch_size]\n",
    "        \n",
    "        pad_question_batch = pad_sentence_batch(questions_batch, vocab_identifiers)\n",
    "        pad_answer_batch = pad_sentence_batch(answers_batch, vocab_identifiers)\n",
    "        \n",
    "        yield pad_question_batch, pad_answer_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(int_questions_sorted, int_answers_sorted, validation_size):\n",
    "    '''\n",
    "    # TODO: shuffle sort of questions prior to split\n",
    "    Produces train test split on questions and answers.\n",
    "    INPUT\n",
    "        int_questions_sorted: sorted list of integers and <UNK> tokens, the questions.\n",
    "        int_answers_sorted: sorted list of integers and <UNK> tokens, the answers.\n",
    "        validation_size: float. Value between 0 and 1 representing percent of data to be\n",
    "            held out for validation.\n",
    "    OUTPUT\n",
    "        training_questions: list of integers. Training set of questions.\n",
    "        training_answers: list of integers. Training set of answers.\n",
    "        validation_questions: list of integers. Validation set of questions.\n",
    "        validation_answers: list of integers. Validation set of answers.\n",
    "    '''\n",
    "    validation_count = int(len(int_questions_sorted) * validation_size)\n",
    "    \n",
    "    training_questions = int_questions_sorted[validation_count:]\n",
    "    validation_questions = int_questions_sorted[:validation_count]\n",
    "    \n",
    "    training_answers = int_answers_sorted[validation_count:]\n",
    "    validation_answers = int_answers_sorted[:validation_count]\n",
    "    \n",
    "    return training_questions, training_answers, validation_questions, validation_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.2: Modeling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_inputs():\n",
    "    '''\n",
    "    creates placeholders for model input\n",
    "    '''\n",
    "    input_data = tf.placeholder(tf.int32, shape=[None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, shape=[None, None], name='targets')\n",
    "    learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    drop_prob = tf.placeholder(tf.float32, name='drop_prob')\n",
    "    \n",
    "    return input_data, targets, learning_rate, drop_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_encoding_input(targets, vocab_identifiers, batch_size):\n",
    "    '''\n",
    "    Remove the last word id from each batch and add <GO> to the beginning\n",
    "    of each batch\n",
    "    INPUT\n",
    "        targets: tensorflow placeholder. Target variable.\n",
    "        vocab_identifier: dictionary. Maps each word to a unique integer.\n",
    "        batch_size: integer. Number of samples to run through model at a time.\n",
    "    OUTPUT\n",
    "        formatted_input: tensorflow tensor. sentences starting with wordid (int)\n",
    "            for <GO>\n",
    "    '''\n",
    "    slice_off_the_end = tf.strided_slice(targets, [0,0], [batch_size, -1], [1,1])\n",
    "    formatted_input = tf.concat(\n",
    "        values=[tf.fill([batch_size, 1], vocab_identifiers['<GO>']), slice_off_the_end],\n",
    "        axis=1)\n",
    "    return formatted_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoding_layer(\n",
    "    input_data, \n",
    "    lstm_unit_count, \n",
    "    num_layers, \n",
    "    drop_prob, \n",
    "    sequence_length\n",
    "):\n",
    "    '''\n",
    "    Creates multilayer bidirectional Recurrent Neural Net encoding.\n",
    "    INPUT\n",
    "        input_data: tensorflow tensor. input_data created by create_model_input function.\n",
    "        lstm_unit_count: integer. The number of lstm units.\n",
    "        num_layers: integer. Number of layers.\n",
    "        drop_prob: float between 0 and 1. Probability of dropping a hidden unit in training\n",
    "        sequence_length: vector of ints where each integer represents the length \n",
    "            of that sequence.\n",
    "    OUTPUT\n",
    "        states: tuple of backward and forward final states of RNN.\n",
    "    '''\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(num_units=lstm_unit_count)\n",
    "    # dropout regularization\n",
    "    dropout = tf.contrib.rnn.DropoutWrapper(lstm_cell, input_keep_prob=(1-drop_prob))\n",
    "    multi_rnn_cell = tf.contrib.rnn.MultiRNNCell([dropout] * num_layers)\n",
    "    # Don't care about outputs because we are feeding this into a decoding layer\n",
    "    outputs, states = tf.nn.bidirectional_dynamic_rnn(\n",
    "        cell_fw=multi_rnn_cell,\n",
    "        cell_bw=multi_rnn_cell,\n",
    "        sequence_length=sequence_length,\n",
    "        inputs=input_data,\n",
    "        dtype=tf.float32\n",
    "    )\n",
    "    return states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_decoding_layer(\n",
    "    encoder_state, \n",
    "    decoding_cell, \n",
    "    dec_embed_input, \n",
    "    sequence_length, \n",
    "    decoding_scope,\n",
    "    output_fn, \n",
    "    drop_prob, \n",
    "    batch_size\n",
    "):\n",
    "    '''\n",
    "    Trains decoding layer for RNN\n",
    "    INPUT\n",
    "        encoder state: Tuple. Output of create_encoding_layer.\n",
    "        decode_cell: multiRNNCell.\n",
    "        dec_embed_input: tensorflow variable. decoder embedding.\n",
    "        sequence_length: integer. The max sentence length for each batch.\n",
    "        decoding_scope: tensorflow variable scope.\n",
    "        output_fn: tensorflow fully connected layer.\n",
    "        drop_prob: float between 0 and 1. Probability of dropping a hidden unit in training.\n",
    "        batch_size: integer. Number of samples to run through the model at a time.\n",
    "    OUTPUT\n",
    "        decoding_layer_output: tensorflow tensor. \n",
    "        \n",
    "    '''\n",
    "    \n",
    "    attention_states = tf.zeros([batch_size, 1, decoding_cell.output_size])\n",
    "    \n",
    "    att_keys, att_vals, att_score_fn, att_construct_fn = (\n",
    "        tf.contrib.seq2seq\n",
    "        .prepare_attention( # TODO : Update to Wrapper\n",
    "            attention_states, \n",
    "            attention_option=\"bahdanau\", \n",
    "            num_units=decoding_cell.output_size\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    train_decoder_fn = (\n",
    "        tf.contrib.seq2seq\n",
    "        .attention_decoder_fn_train(\n",
    "            encoder_state[0],\n",
    "            att_keys,\n",
    "            att_vals,\n",
    "            att_score_fn,\n",
    "            att_construct_fn,\n",
    "            name = \"attn_dec_train\"\n",
    "        )\n",
    "    )\n",
    "    train_pred, _, _ = (\n",
    "        tf.contrib.seq2seq\n",
    "        .dynamic_rnn_decoder(\n",
    "            cell=decoding_cell, \n",
    "            decoder_fn=train_decoder_fn, \n",
    "            inputs=dec_embed_input, \n",
    "            sequence_length=sequence_length, \n",
    "            scope=decoding_scope\n",
    "        )\n",
    "    )\n",
    "    train_pred_drop = tf.nn.dropout(train_pred, 1-drop_prob)\n",
    "    decoding_layer_output = output_fn(train_pred_drop)\n",
    "    return decoding_layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_decoding_layer(\n",
    "    encoder_state, \n",
    "    decode_cell, \n",
    "    dec_embeddings, \n",
    "    seq_start_id, \n",
    "    seq_end_id,\n",
    "    max_length,\n",
    "    vocab_size, \n",
    "    decoding_scope, \n",
    "    output_fn,  \n",
    "    batch_size):\n",
    "    '''\n",
    "    Performs decoding inference on data run through RNN decoder.\n",
    "    INPUT\n",
    "        encoder_state: Tuple. Output of create_encoding_layer.\n",
    "        decode_cell: multiRNNCell.\n",
    "        dec_embeddings: np.array. Matrix of decoder embeddings.\n",
    "        seq_start_id: string. id signaling new phrase. In this case, <GO>\n",
    "        seq_end_id: string. is signaling end of phrase. In this case, <EOS>\n",
    "        max_length: integer. Max number of timesteps allowable to decode.\n",
    "        vocab_size: integer. Size of vocabulary.\n",
    "        decoding_scope: tensorflow variable_scope.\n",
    "        output_fn: function. Output function to project cell output onto class logits.\n",
    "        batch_size: integer. Number of samples to run through model at a time.\n",
    "    OUTPUT\n",
    "        infer_logits: tensorflow tensor. Inferred value.\n",
    "    '''\n",
    "    \n",
    "    attention_states = tf.zeros([batch_size, 1, dec_cell.output_size])\n",
    "    \n",
    "    att_keys, att_vals, att_score_fn, att_construct_fn = \\\n",
    "            tf.contrib.seq2seq.prepare_attention( # TODO: Update to attention wrapper\n",
    "                attention_states,\n",
    "                attention_option=\"bahdanau\",\n",
    "                num_units=dec_cell.output_size)\n",
    "    \n",
    "    infer_decoder_fn = (\n",
    "        tf\n",
    "        .contrib\n",
    "        .seq2seq\n",
    "        .attention_decoder_fn_inference(\n",
    "            output_fn=output_fn, \n",
    "            encoder_state=encoder_state[0], \n",
    "            attention_keys=att_keys, \n",
    "            attention_values=att_vals, \n",
    "            attention_score_fn=att_score_fn, \n",
    "            attention_construct_function=att_construct_fn, \n",
    "            embeddings=dec_embeddings,\n",
    "            start_of_sequence_id=seq_start_id, \n",
    "            end_of_sequence_id=seq_end_id, \n",
    "            maximum_length=max_length, \n",
    "            num_decoder_symbols=vocab_size, \n",
    "            name = \"attn_dec_inf\"\n",
    "        )\n",
    "    )\n",
    "    infer_logits, _, _ = (\n",
    "        tf\n",
    "        .contrib\n",
    "        .seq2seq\n",
    "        .dynamic_rnn_decoder(\n",
    "            dec_cell,\n",
    "            infer_decoder_fn,\n",
    "            scope=decoding_scope\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return infer_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer(\n",
    "    dec_embed_input, \n",
    "    dec_embeddings, \n",
    "    encoder_state, \n",
    "    vocab_size, \n",
    "    sequence_length, \n",
    "    lstm_unit_count,\n",
    "    num_layers, \n",
    "    int_lto_word_dict, \n",
    "    drop_prob, \n",
    "    batch_size):\n",
    "    '''\n",
    "    Creates the decoding cell that runs decoding training and inference.\n",
    "    INPUT:\n",
    "        dec_embed_input: Tensorflow variable. decoder embedding.\n",
    "        dec_embeddings: np.array. Matrix of decoder embeddings.\n",
    "        encoder_state: Tuple. Output of create_encoding_layer.\n",
    "        vocab_size: Integer. Number of words in vocabulary.\n",
    "        sequence_length: Integer. The max sentence length for each batch.\n",
    "        lstm_unit_count: Integer. Number of units in LSTM cell.\n",
    "        num_layers: Integer. Number of layers in RNN cell.\n",
    "        int_to_word_dict: Dictionary mapping words to integers.\n",
    "        drop_prob: Float between 0 and 1. Probability of dropping a hidden unit in training.\n",
    "        batch_size: Integer. Number of samples to run through model at a time.\n",
    "    OUTPUT:\n",
    "        train_logits: Tensorflow tensor. Trained values.\n",
    "        infer_logits: Tensorflow tensor. Infered values.\n",
    "    '''\n",
    "    with tf.variable_scope(\"decoding\") as decoding_scope:\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_unit_count)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = 1 - drop_prob)\n",
    "        dec_cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "\n",
    "        weights = tf.truncated_normal_initializer(stddev=0.1)\n",
    "        biases = tf.zeros_initializer()\n",
    "        output_fn = lambda x: tf.contrib.layers.fully_connected(\n",
    "            inputs=x, \n",
    "            num_outputs=vocab_size, \n",
    "            activation_fn=None, \n",
    "            scope=decoding_scope,\n",
    "            weights_initializer=weights,\n",
    "            biases_initializer=biases\n",
    "        )\n",
    "\n",
    "        train_logits = train_decoding_layer(\n",
    "            encoder_state, \n",
    "            dec_cell, \n",
    "            dec_embed_input, \n",
    "            sequence_length, \n",
    "            decoding_scope, \n",
    "            output_fn, \n",
    "            drop_prob, \n",
    "            batch_size\n",
    "        )\n",
    "        decoding_scope.reuse_variables()\n",
    "        \n",
    "        infer_logits = infer_decoding_layer(\n",
    "            encoder_state, \n",
    "            dec_cell, \n",
    "            dec_embeddings, \n",
    "            int_to_word_dict['<GO>'],\n",
    "            int_to_word_dict['<EOS>'], \n",
    "            sequence_length - 1, \n",
    "            vocab_size,\n",
    "            decoding_scope, \n",
    "            output_fn, \n",
    "            drop_prob, \n",
    "            batch_size\n",
    "        )\n",
    "        \n",
    "    return train_logits, infer_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(\n",
    "    input_data, \n",
    "    target_data, \n",
    "    drop_prob, \n",
    "    batch_size, \n",
    "    sequence_length, \n",
    "    answers_vocab_size, \n",
    "    questions_vocab_size, \n",
    "    enc_embedding_size,  \n",
    "    rnn_size, \n",
    "    num_layers, \n",
    "    vocab_identifiers): \n",
    "    '''\n",
    "    Use the previous functions to create the training and inference logits\n",
    "    INPUT\n",
    "        input_data: Tensorflow placeholder for input to model.\n",
    "        target_data: Tensorflow placeholder for target variable.\n",
    "        drop_prob: Float between 0 and 1. Probability of dropping a hidden unit in training. \n",
    "        batch_size: Integer. Number of samples to run through model at a time.\n",
    "        sequence_length: Integer. The max sentence length for each batch.\n",
    "        answers_vocab_size: Integer. Number of unique words in answers.\n",
    "        questions_vocab_size: Integer. Number of unique words in questions.\n",
    "        enc_embedding_size: Integer. Number of dimensions for embedding matrix.\n",
    "        rnn_size: Integer. Number of units in LSTM cell.\n",
    "        num_layers: Integer. Number of layers in RNN cell. \n",
    "        vocab_identifiers: Dictionary. Maps each word to a unique integer.\n",
    "    OUTPUT\n",
    "        train_logits: Tensorflow tensor. Trained values.\n",
    "        infer_logits: Tensorflow tensor. Infered values.\n",
    "    '''\n",
    "    \n",
    "    # Create embedding from encoder\n",
    "    enc_embed_input = tf.contrib.layers.embed_sequence(\n",
    "        ids=input_data, \n",
    "        vocab_size=answers_vocab_size + 1, \n",
    "        embed_dim=enc_embedding_size,\n",
    "        initializer=tf.random_uniform_initializer(0,1)\n",
    "    )\n",
    "    enc_state = create_encoding_layer(\n",
    "        enc_embed_input, \n",
    "        rnn_size, \n",
    "        num_layers, \n",
    "        drop_prob, \n",
    "        sequence_length\n",
    "    )\n",
    "\n",
    "    # Decode embedded data\n",
    "    dec_input = process_encoding_input(target_data, vocab_identifiers, batch_size)\n",
    "    dec_embeddings = tf.Variable(\n",
    "        tf.random_uniform([questions_vocab_size + 1, enc_embedding_size], 0, 1)\n",
    "    )\n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "    \n",
    "    train_logits, infer_logits = decoding_layer(\n",
    "        dec_embed_input, \n",
    "        dec_embeddings, \n",
    "        enc_state, \n",
    "        questions_vocab_size, \n",
    "        sequence_length, \n",
    "        rnn_size, \n",
    "        num_layers, \n",
    "        int_form_questions, \n",
    "        drop_prob, \n",
    "        batch_size\n",
    "    )\n",
    "    return train_logits, infer_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cost_function(\n",
    "    sequence_length,\n",
    "    input_shape,\n",
    "    train_logits,\n",
    "    targets,\n",
    "    learning_rate):\n",
    "    '''\n",
    "    Creates cost function and applies clipped gradient backpropogation\n",
    "    to model while in training.\n",
    "    INPUT\n",
    "        sequence_length: Integer.\n",
    "        input_shape: Tesnroflow tensor. Shape of input data.\n",
    "        train_logits: Tensorflow tensor. Trained values.\n",
    "        targets: Tensorflow placeholder. Target variable.\n",
    "        learning_rate: Float. Learning Rate.\n",
    "    OUTPUT\n",
    "        training_optimizer: Tensorflow operation. Applies gradients to model using \n",
    "            Adam optimizer.\n",
    "    '''\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            train_logits,\n",
    "            targets,\n",
    "            tf.ones([input_shape[0], sequence_length])\n",
    "        )\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        clipped_gradients = [\n",
    "            (tf.clip_by_value(grad, clip_value_min=-5.0, clip_value_max=5.0), var) \n",
    "            for grad, var in gradients \n",
    "            if grad is not None\n",
    "        ]\n",
    "        training_optimizer = optimizer.apply_gradients(clipped_gradients)\n",
    "\n",
    "        return training_optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines, conv_lines = load_in_lines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_mapping = create_line_mapping(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "convs = format_conv_lines(conv_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions, answers = create_questions_and_answers(convs, line_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_questions, clean_answers = create_clean_questions_and_answers(questions, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_filtered, answers_filtered = \\\n",
    "    filter_long_and_short_sentences(clean_questions, clean_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_freq = create_word_frequency_dictionaries(answers_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_vocab = filter_uncommon_words(vocab_freq, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_identifiers = create_unique_identifiers(filtered_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifiers_with_tokens = create_dictionary_tokens(vocab_identifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_word_dict = create_int_to_word_dict(identifiers_with_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_filtered_eos = add_eos_token(answers_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_form_questions = convert_lines_to_ints(questions_filtered, vocab_identifiers)\n",
    "int_form_answers = convert_lines_to_ints(answers_filtered_eos, identifiers_with_tokens)\n",
    "answers_vocab_size = len(int_form_answers)\n",
    "questions_vocab_size = len(int_form_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4227 unique words\n",
      "Percent of total words spoken that are unkown: 6.9%\n"
     ]
    }
   ],
   "source": [
    "get_percent_unk(int_form_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_questions_sorted, int_answers_sorted = \\\n",
    "    sort_by_question_length(int_form_questions, int_form_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_questions, training_answers, validation_questions, validation_answers = \\\n",
    "    train_test_split(int_questions_sorted, int_answers_sorted, 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.2: Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Hyperparameters\n",
    "batch_size = 128\n",
    "decoding_embedding_size = 512\n",
    "drop_prob = 0.25\n",
    "encoding_embedding_size = 512\n",
    "epochs = 10\n",
    "learning_rate = 0.005\n",
    "learning_rate_decay = 0.9\n",
    "lstm_unit_count = 512\n",
    "min_learning_rate = 0.0001\n",
    "min_sentencelength=2\n",
    "max_sentence_length=20\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data, targets, learning_rate, drop_prob = create_model_inputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_input = process_encoding_input(targets, vocab_identifiers, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reset the graph to ensure that it is ready for training\n",
    "# tf.reset_default_graph()\n",
    "session_config = tf.ConfigProto(device_count={'CPU': 10})\n",
    "sess = tf.InteractiveSession(config=session_config)\n",
    "# Sequence length will be the max line length for each batch\n",
    "sequence_length = tf.placeholder_with_default(max_sentence_length, None, name='sequence_length')\n",
    "input_shape = tf.shape(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin debugging here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'prepare_attention'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-87dcc7ab6140>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mlstm_unit_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     vocab_identifiers)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-bdd96685ca8a>\u001b[0m in \u001b[0;36mseq2seq_model\u001b[0;34m(input_data, target_data, drop_prob, batch_size, sequence_length, answers_vocab_size, questions_vocab_size, enc_embedding_size, rnn_size, num_layers, vocab_identifiers)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mint_form_questions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mdrop_prob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     )\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfer_logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-9063fde6301f>\u001b[0m in \u001b[0;36mdecoding_layer\u001b[0;34m(dec_embed_input, dec_embeddings, encoder_state, vocab_size, sequence_length, lstm_unit_count, num_layers, int_lto_word_dict, drop_prob, batch_size)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0moutput_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mdrop_prob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         )\n\u001b[1;32m     55\u001b[0m         \u001b[0mdecoding_scope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreuse_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-b613da14908e>\u001b[0m in \u001b[0;36mtrain_decoding_layer\u001b[0;34m(encoder_state, decoding_cell, dec_embed_input, sequence_length, decoding_scope, output_fn, drop_prob, batch_size)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     att_keys, att_vals, att_score_fn, att_construct_fn = (\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq2seq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         .prepare_attention(\n\u001b[1;32m     32\u001b[0m             \u001b[0mattention_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'prepare_attention'"
     ]
    }
   ],
   "source": [
    "train_logits, infer_logits = seq2seq_model(    \n",
    "    input_data, \n",
    "    targets, \n",
    "    drop_prob, \n",
    "    batch_size, \n",
    "    sequence_length, \n",
    "    answers_vocab_size, \n",
    "    questions_vocab_size, \n",
    "    encoding_embedding_size,  \n",
    "    lstm_unit_count, \n",
    "    num_layers, \n",
    "    vocab_identifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.1\n"
     ]
    }
   ],
   "source": [
    "print tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_function = create_cost_function(sequence_length, input_shape, train_logits, targets, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_step = 100 # Check training loss after every 100 batches\n",
    "stop_early = 0 \n",
    "stop = 5 # If the validation loss does decrease in 5 consecutive checks, stop training\n",
    "validation_check = ((len(training_questions)) // batch_size // 2) - 1 # Modulus for checking validation loss\n",
    "total_train_loss = 0 # Record the training loss for each display step\n",
    "summary_valid_loss = [] # Record the validation loss for saving improvements in the model\n",
    "\n",
    "checkpoint = \"best_model.ckpt\" \n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch_i in range(1, epochs + 1):\n",
    "    for batch_i, (batch_questions, batch_answers) in enumerate(\n",
    "        batch_data((int_form_questions, int_form_answers, batch_size, vocab_identifiers))\n",
    "    ):\n",
    "        start_time = time.time()\n",
    "        _, loss = sess.run(\n",
    "            [train_op, cost]\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "deep_learning_top",
   "language": "python",
   "name": "deep_learning_top"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
