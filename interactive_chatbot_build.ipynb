{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is this notebook?\n",
    "This notebook takes in movie conversations and trains a chatbot to interact with people. This chatbot is trained via a tensorflow neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.1: Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_in_lines():\n",
    "    with open('data/dialogues/movie_lines.txt', 'r') as f:\n",
    "        lines = f.read().split('\\n')\n",
    "    with open('data/dialogues/movie_conversations.txt', 'r') as g:\n",
    "        conv_lines = g.read().split('\\n')\n",
    "    return lines, conv_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_line_mapping(lines):\n",
    "    '''\n",
    "    creates a dictionary mapping lineids to the lines.\n",
    "    '''\n",
    "    line_mapping = {}\n",
    "    for line in lines:\n",
    "        split_line = line.split(' +++$+++ ')\n",
    "        if len(split_line) == 5:\n",
    "            line_mapping[split_line[0]] = split_line[4]\n",
    "    return line_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_conv_lines(conv_lines):\n",
    "    '''\n",
    "    extracts just the line ids from the conv_lines list.\n",
    "    returns a list of lists of strings. Each string is a lineid.\n",
    "    '''\n",
    "    convs = []\n",
    "    for conv in conv_lines[:-1]:\n",
    "        conv_line_list = conv.split(\" +++$+++ \")[-1]\n",
    "        convs.append(ast.literal_eval(conv_line_list))\n",
    "    return convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_questions_and_answers(convs, line_mapping):\n",
    "    '''\n",
    "    creates a list of questions and a list of answers.\n",
    "    These are the back to back lines from each conversation.\n",
    "    '''\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for conv in convs:\n",
    "        for i in range(len(conv) - 1):\n",
    "            questions.append(line_mapping[conv[i]])\n",
    "            answers.append(line_mapping[conv[i+1]])\n",
    "    return questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_clean_questions_and_answers(questions, answers):\n",
    "    '''\n",
    "    applies clean_text function to questions and answers lists.\n",
    "    '''\n",
    "    clean_questions = [clean_text(text) for text in questions]\n",
    "    clean_answers = [clean_text(text) for text in answers]\n",
    "    return clean_questions, clean_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_long_and_short_sentences(questions, answers, min_threshold=2, max_threshold=20):\n",
    "    '''\n",
    "    Filters out questions and answers with length below the min_threshold (2)\n",
    "    and above the max_threshold (20)\n",
    "    '''\n",
    "    questions_filtered = []\n",
    "    answers_filtered = []\n",
    "    \n",
    "    for q, a in zip(questions, answers):\n",
    "        if len(q.split()) > min_threshold and len(q.split()) < max_threshold and \\\n",
    "        len(a.split()) > min_threshold and len(a.split()) < max_threshold:\n",
    "            questions_filtered.append(q)\n",
    "            answers_filtered.append(a)\n",
    "   \n",
    "    return questions_filtered, answers_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_word_frequency_dictionaries(sentence_list):\n",
    "    '''\n",
    "    This function applies to either the list of questions of the list of answers.\n",
    "    It creates a dictionary mapping words to the number of times that word occurs.\n",
    "    '''\n",
    "    word_frequencies = {}\n",
    "    for sentence in sentence_list:\n",
    "        for word in sentence.split():\n",
    "            if word_frequencies.get(word, 0) == 0:\n",
    "                word_frequencies[word] = 1\n",
    "            else:\n",
    "                word_frequencies[word] += 1\n",
    "    return word_frequencies    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_uncommon_words(vocab, threshold):\n",
    "    '''\n",
    "    filters out words from vocab dictionary (output of create_word_frequency_dictionaries)\n",
    "    that occurs fewer than threshold many times.\n",
    "    '''\n",
    "    filtered_vocab = {key: val for key, val in vocab.iteritems() if val >= threshold}\n",
    "    return filtered_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_unique_identifiers(vocab):\n",
    "    '''\n",
    "    creates dictionary mapping each word in vocab to a unique number\n",
    "    '''\n",
    "    words = set(vocab.keys())\n",
    "    vocab_identifiers = {}\n",
    "    unique_int = 0\n",
    "    for word in words:\n",
    "        vocab_identifiers[word] = unique_int\n",
    "        unique_int += 1\n",
    "    \n",
    "    return vocab_identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dictionary_tokens(vocab_identifiers):\n",
    "    '''\n",
    "    adds identification tokens to the vocab_identifiers dictionary\n",
    "    '''\n",
    "    codes = ['<PAD>','<EOS>','<UNK>','<GO>']\n",
    "\n",
    "    for code in codes:\n",
    "        vocab_identifiers[code] = len(filtered_vocab)+1\n",
    "    \n",
    "    # Rename vocab_identifiers now this has tokens. This is just for interpretability reasons.\n",
    "    identifiers_with_tokens = vocab_identifiers\n",
    "    return identifiers_with_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_int_to_word_dict(identifiers_with_tokens):\n",
    "    '''\n",
    "    creates a dictionary mapping integers to words as opposed to words mapping to \n",
    "    integers.\n",
    "    '''\n",
    "    int_to_word_dict = {idx : word for word, idx in identifiers_with_tokens.iteritems()}\n",
    "    return int_to_word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_eos_token(lines):\n",
    "    '''\n",
    "    Here lines is a list of strings (sentences). This will generally be either\n",
    "        questions_filtered (or)\n",
    "        answers_filtered\n",
    "    but any input of form list of strings will word.\n",
    "    This function appends ' <EOS>' to the end of each string, representing the\n",
    "    end of sentence.\n",
    "    '''\n",
    "    lines_with_eos = [x + ' <EOS>' for x in lines]\n",
    "    return lines_with_eos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_lines_to_ints(lines, word_identifiers):\n",
    "    '''\n",
    "    converts list of lines to a list of list of integers where each integer represents\n",
    "    a word. The integer mapping is per the word_identifiers defined by the \n",
    "    create_unique_identifiers function. If a word isn't in the word_identifiers, it will be\n",
    "    replaced with '<UNK>' for unknown.\n",
    "    INPUT\n",
    "        lines: list of strings. Each string is a movie line.\n",
    "        word_identifiers: dictionary. maps words to unique identifiers. \n",
    "    OUTPUT\n",
    "        int_lines: list of list of integers.\n",
    "    '''\n",
    "    int_lines = []\n",
    "    for line in lines:\n",
    "        word_split = line.split()\n",
    "        words_as_ints = map(lambda x: word_identifiers.get(x, '<UNK>'), word_split)\n",
    "        int_lines.append(words_as_ints)\n",
    "    \n",
    "    return int_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_percent_unk(int_lines):\n",
    "    '''\n",
    "    tells us the percent of total words in int_lines that are the '<UNK>' token.\n",
    "    INPUT\n",
    "        int_lines: list of list of integers and <UNK> tokens. Output of convert_lines_to_ints\n",
    "        function.\n",
    "    '''\n",
    "    words_flat = [word for sentence in int_lines for word in sentence]\n",
    "    word_set = set(words_flat)\n",
    "    with_unk = len(words_flat)\n",
    "    without_unk = len([x for x in words_flat if type(x) == int])\n",
    "    percent_unk = (with_unk - float(without_unk)) / with_unk\n",
    "    \n",
    "    print \"{} unique words\".format(len(word_set))\n",
    "    print \"Percent of total words spoken that are unkown: {}%\".format(round(100*percent_unk, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sort_by_question_length(int_form_questions, int_form_answers):\n",
    "    '''\n",
    "    Sorts question and answer pairs by the lengths of the questions.\n",
    "    Implementing this function speeds up training time and reduces loss.\n",
    "    INPUT\n",
    "        int_form_questions: list of integers and <UNK> tokens, the questions.\n",
    "        int_form_answers: list of integers and <UNK> tokens, the answers.\n",
    "    OUTPUT\n",
    "        int_questions_sorted: sorted list of integers and <UNK> tokens, the questions.\n",
    "        int_answers_sorted: sorted list of integers and <UNK> tokens, the answers.\n",
    "    '''\n",
    "    indices = np.argsort([len(sent) for sent in int_form_questions])\n",
    "    int_questions_sorted = list(np.array(int_form_questions)[indices])\n",
    "    int_answers_sorted = list(np.array(int_form_answers)[indices])\n",
    "    return int_questions_sorted, int_answers_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.2: Modeling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model_inputs():\n",
    "    '''\n",
    "    creates placeholders for model input\n",
    "    '''\n",
    "    input_data = tf.placeholder(tf.int32, shape=None, name='input')\n",
    "    targets = tf.placeholder(tf.int32, shape=None, name='targets')\n",
    "    learning_rate = tf.placeholder(tf.float32, shape=None, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, shape=None, name='keep_prob')\n",
    "    \n",
    "    return input_data, targets, learning_rate, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_encoding_input(targets, word_identifiers, batch_size):\n",
    "    '''\n",
    "    Remove the last word id from each batch and add <GO> to the beginning\n",
    "    of each batch\n",
    "    INPUT\n",
    "        targets: tensorflow placeholder. Target variable.\n",
    "        word_identifier: dictionary. Maps each word to a unique integer.\n",
    "        batch_size: integer. Number of samples to run through model at a time.\n",
    "    OUTPUT\n",
    "        formatted_input: tensorflow tensor. sentences starting with wordid (int)\n",
    "            for <GO>\n",
    "    '''\n",
    "    slice_off_the_end = tf.strided_slice(targets, [0,0], [batch_size, -1], [1,1])\n",
    "    formatted_input = tf.concat(\n",
    "        [tf.fill([batch_size, 1], word_identifiers['<GO>']), \n",
    "        slice_off_the_end],\n",
    "        axis=1)\n",
    "    return formatted_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_encoding_layer(\n",
    "    rnn_inputs, \n",
    "    lstm_unit_count, \n",
    "    num_layers, \n",
    "    drop_prob, \n",
    "    sequence_length\n",
    "):\n",
    "    '''\n",
    "    Creates multilayer bidirectional Recurrent Neural Net encoding.\n",
    "    INPUT\n",
    "        rnn_inputs: tensorflow tensor. input_data created by create_model_input function.\n",
    "        lstm_unit_count: integer. The number of lstm units.\n",
    "        num_layers: integer. Number of layers.\n",
    "        drop_prob: float between 0 and 1. Probability of dropping a hidden unit in training\n",
    "        sequence_length: vector of ints where each integer represents the length \n",
    "            of that sequence.\n",
    "    OUTPUT\n",
    "        states: tuple of backward and forward final states of RNN.\n",
    "    '''\n",
    "    lstm_cell = lstm.tf.contrib.rnn.BasicLSTMCell(num_units=lstm_unit_count)\n",
    "    # dropout regularization\n",
    "    dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob=(1-drop_prob))\n",
    "    multi_rnn_cell = tf.contrib.rnn.MultiRNNCell([dropout] * num_layers)\n",
    "    # Don't care about outputs because we are feeding this into a decoding layer\n",
    "    outputs, states = tf.nn.bidirectional_dynamic_rnn(\n",
    "        cell_fw=multi_rnn_cell,\n",
    "        cell_bw=multi_rnn_cell,\n",
    "        sequence_length=sequence_length,\n",
    "        inputs=rnn_inputs,\n",
    "        dtype=tf.float32\n",
    "    )\n",
    "    return states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_decoding_layer(\n",
    "    encoder_state, \n",
    "    decoding_cell, \n",
    "    dec_embed_input, \n",
    "    sequence_length, \n",
    "    decoding_scope,\n",
    "    output_fn, \n",
    "    drop_prob, \n",
    "    batch_size\n",
    "):\n",
    "    '''\n",
    "    Creates decoding layer for RNN\n",
    "    INPUT\n",
    "        encoder state: Tuple. Output of create_encoding_layer.\n",
    "        decode_cell: multiRNNCell.\n",
    "        dec_embed_input: tensorflow variable. decoder embedding.\n",
    "        sequence_length: integer. The max sentence length for each batch.\n",
    "        decoding_scope: tensorflow variable scope.\n",
    "        output_fn: tensorflow fully connected layer.\n",
    "        drop_prob: float between 0 and 1. Probability of dropping a hidden unit in training.\n",
    "        batch_size: integer. Number of samples to run through the model at a time.\n",
    "    OUTPUT\n",
    "        decoding_layer_output: tensorflow tensor. \n",
    "        \n",
    "    '''\n",
    "    \n",
    "    attention_states = tf.zeros([batch_size, 1, decoding_cell.output_size])\n",
    "    \n",
    "    att_keys, att_vals, att_score_fn, att_construct_fn = (\n",
    "        tf.contrib.seq2seq\n",
    "        .prepare_attention(\n",
    "            attention_states, \n",
    "            attention_option=\"bahdanau\", \n",
    "            num_units=decoding_cell.output_size\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    train_decoder_fn = (\n",
    "        tf.contrib.seq2seq\n",
    "        .attention_decoder_fn_train(\n",
    "            encoder_state[0],\n",
    "            att_keys,\n",
    "            att_vals,\n",
    "            att_score_fn,\n",
    "            att_construct_fn,\n",
    "            name = \"attn_dec_train\"\n",
    "        )\n",
    "    )\n",
    "    train_pred, _, _ = (\n",
    "        tf.contrib.seq2seq\n",
    "        .dynamic_rnn_decoder(\n",
    "            decoding_cell, \n",
    "            train_decoder_fn, \n",
    "            dec_embed_input, \n",
    "            sequence_length, \n",
    "            scope=decoding_scope\n",
    "        )\n",
    "    )\n",
    "    train_pred_drop = tf.nn.dropout(train_pred, 1-drop_prob)\n",
    "    decoding_layer_output = output_fn(train_pred_drop)\n",
    "    return decoding_layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoding_layer_infer(\n",
    "    encoder_state, \n",
    "    dec_cell, \n",
    "    dec_embeddings, \n",
    "    seq_start_id, \n",
    "    seq_end_id,\n",
    "    max_length,\n",
    "    vocab_size, \n",
    "    decoding_scope, \n",
    "    output_fn,  \n",
    "    batch_size):\n",
    "    '''\n",
    "    Decode the prediction data\n",
    "    INPUT\n",
    "        encoder_state:\n",
    "        dec_cell:\n",
    "        dec_embeddings:\n",
    "        seq_start_id:\n",
    "        seq_end_id:\n",
    "        max_length:\n",
    "        vocab_size:\n",
    "        decoding_scope:\n",
    "        output_fn:\n",
    "        batch_size:\n",
    "    OUTPUT\n",
    "        infer_logits:\n",
    "    '''\n",
    "    \n",
    "    attention_states = tf.zeros([batch_size, 1, dec_cell.output_size])\n",
    "    \n",
    "    att_keys, att_vals, att_score_fn, att_construct_fn = \\\n",
    "            tf.contrib.seq2seq.prepare_attention(attention_states,\n",
    "                                                 attention_option=\"bahdanau\",\n",
    "                                                 num_units=dec_cell.output_size)\n",
    "    \n",
    "    infer_decoder_fn = tf.contrib.seq2seq.attention_decoder_fn_inference(output_fn, \n",
    "                                                                         encoder_state[0], \n",
    "                                                                         att_keys, \n",
    "                                                                         att_vals, \n",
    "                                                                         att_score_fn, \n",
    "                                                                         att_construct_fn, \n",
    "                                                                         dec_embeddings,\n",
    "                                                                         seq_start_id, \n",
    "                                                                         seq_end_id, \n",
    "                                                                         max_length, \n",
    "                                                                         vocab_size, \n",
    "                                                                         name = \"attn_dec_inf\")\n",
    "    infer_logits, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(dec_cell, \n",
    "                                                                infer_decoder_fn, \n",
    "                                                                scope=decoding_scope)\n",
    "    \n",
    "    return infer_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines, conv_lines = load_in_lines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "line_mapping = create_line_mapping(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "convs = format_conv_lines(conv_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "questions, answers = create_questions_and_answers(convs, line_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_questions, clean_answers = create_clean_questions_and_answers(questions, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "questions_filtered, answers_filtered = \\\n",
    "    filter_long_and_short_sentences(clean_questions, clean_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_freq = create_word_frequency_dictionaries(answers_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_vocab = filter_uncommon_words(vocab_freq, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_identifiers = create_unique_identifiers(filtered_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "identifiers_with_tokens = create_dictionary_tokens(word_identifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "int_to_word_dict = create_int_to_word_dict(identifiers_with_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answers_filtered_eos = add_eos_token(answers_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "int_form_questions = convert_lines_to_ints(questions_filtered, word_identifiers)\n",
    "int_form_answers = convert_lines_to_ints(answers_filtered_eos, identifiers_with_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4227 unique words\n",
      "Percent of total words spoken that are unkown: 6.9%\n"
     ]
    }
   ],
   "source": [
    "get_percent_unk(int_form_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "int_questions_sorted, int_answers_sorted = \\\n",
    "    sort_by_question_length(int_form_questions, int_form_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.2: Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the Hyperparameters\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "rnn_size = 512\n",
    "num_layers = 2\n",
    "encoding_embedding_size = 512\n",
    "decoding_embedding_size = 512\n",
    "learning_rate = 0.005\n",
    "learning_rate_decay = 0.9\n",
    "min_learning_rate = 0.0001\n",
    "drop_prob = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_data, targets, learning_rate, keep_prob = create_model_inputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "formatted_input = process_encoding_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_encoding_layer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b7b4221b5b03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_encoding_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'create_encoding_layer' is not defined"
     ]
    }
   ],
   "source": [
    "states = create_encoding_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
